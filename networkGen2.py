#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
batch_character_graphs.py (updated)
-----------------------------------
novel/<genre>/<book_id>.txt â†’ books_GNN / books_base CSV

ë³€ê²½ ì‚¬í•­
* part ê·¸ë˜í”„ë¥¼ final ë…¸ë“œ ì§‘í•©ì— ë§ì¶° ì •ë ¬í•˜ì§€ ì•ŠìŒ
* íŒŒíŠ¸ë³„ ê·¸ë˜í”„ê°€ ì™„ì„±ë˜ëŠ” ì¦‰ì‹œ CSVë¡œ ì €ì¥
"""

# --------------------------------------------------
# 0. ê³µí†µ ì˜ì¡´ì„± ë° ìœ í‹¸ í•¨ìˆ˜(ì›ë³¸ ê·¸ëŒ€ë¡œ)
# --------------------------------------------------
import spacy, time, csv, re, itertools, os, math, json, gc
from collections import defaultdict, Counter

import torch
import networkx as nx
from rapidfuzz import fuzz
from allennlp.predictors.predictor import Predictor
import allennlp_models.coref

print("start")

# GPU ì„¤ì • --------------------------------------------------------------
if torch.cuda.is_available():
    spacy.require_gpu()  # spaCyê°€ GPU ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
    cuda_device = 0      # AllenNLPìš© GPU ID
    print("ğŸŸ¢ GPU detected: spaCy & AllenNLP will use cuda_device=0")
else:
    cuda_device = -1     # CPU fallback
    print("âšªï¸ GPU not found: using CPU")

NUM_PARTS       = 4
MAX_CHUNK_WORDS = 600
COREF_MODEL     = "coref-model.tar.gz"

# --------------------------------------------------
# 1. ëª¨ë¸ ë¡œë“œ -----------------------------------------------------------
# --------------------------------------------------
global_start   = time.time()
small_version  = False
nlp = spacy.load("en_core_web_sm") if small_version else spacy.load("en_core_web_trf")
print(f"[0] spaCy ëª¨ë¸ ë¡œë“œ: {time.time() - global_start:.2f}s")

predictor = Predictor.from_path(COREF_MODEL, cuda_device=cuda_device)
print(f"[0] coref ëª¨ë¸ ë¡œë“œ: {time.time() - global_start:.2f}s")

# --------------------------------------------------
# 2. CSV ì €ì¥ í•¨ìˆ˜ --------------------------------------------------------
# --------------------------------------------------

def save_graph_edges_to_csv(G: nx.Graph, filename: str) -> None:
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    with open(filename, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["source", "target", "weight"])

        written = set()
        for u, v, data in G.edges(data=True):
            writer.writerow([u, v, data.get("weight", 1)])
            written.update([u, v])
        # ë…¸ë“œ ë‹¨ë… ë“±ì¥ì„ ê¸°ë¡í•˜ê³  ì‹¶ìœ¼ë©´ ë‹¤ìŒ ì£¼ì„ í•´ì œ
        # for n in G.nodes():
        #     if n not in written:
        #         writer.writerow([n, "", 0])

# --------------------------------------------------
# 3. ì´ë¦„ ì •ê·œí™”Â·í´ëŸ¬ìŠ¤í„°ë§ ---------------------------------------------
# --------------------------------------------------

def normalize(name: str) -> str:
    name = name.lower()
    name = re.sub(r"[\"',\.\(\)]", "", name)
    name = re.sub(r"'s$", "", name)
    name = re.sub(r"\s+", " ", name).strip()
    tokens = name.split()
    if len(tokens) > 1 and len(set(tokens)) == 1:
        name = tokens[0]
    return "" if "chapter" in name else name

class UnionFind:
    def __init__(self, items):
        self.parent = {x: x for x in items}

    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]

    def union(self, a, b):
        pa, pb = self.find(a), self.find(b)
        if pa != pb:
            self.parent[pb] = pa

    def clusters(self):
        out = defaultdict(set)
        for x in self.parent:
            out[self.find(x)].add(x)
        return out

def cluster_names(names, threshold=90, name_counts=None):
    uf = UnionFind(names)
    name_list = list(names)
    for i, a in enumerate(name_list):
        for b in name_list[i + 1:]:
            if fuzz.token_sort_ratio(a, b) >= threshold or \
               re.search(rf"{re.escape(a)}", b) or re.search(rf"{re.escape(b)}", a):
                uf.union(a, b)
    groups = {}
    for cluster in uf.clusters().values():
        if name_counts:
            canonical = max(cluster, key=lambda n: (name_counts.get(n, 0), -len(n)))
        else:
            multi = [n for n in cluster if len(n.split()) > 1]
            canonical = min(multi or cluster, key=len)
        for n in cluster:
            groups[n] = canonical
    return groups

# --------------------------------------------------
# 4. NLP ê¸°ë°˜ í•¨ìˆ˜ ---------------------------------------------------------
# --------------------------------------------------

def extract_characters(text: str):
    doc = nlp(text)
    return {ent.text for ent in doc.ents if ent.label_ == "PERSON"}


def resolve_coreferences(chunk: str, predictor: Predictor) -> str:
    with torch.no_grad():
        out = predictor.coref_resolved(chunk)
    torch.cuda.empty_cache()
    gc.collect()
    return out


def build_interaction_matrix_window(text: str, valid_chars, window_size=3, stride=1):
    matrix = defaultdict(lambda: defaultdict(int))
    doc = nlp(text)
    sentences = list(doc.sents)
    sent_char_lists = []
    for sent in sentences:
        chars_here = {ent.text for ent in sent.ents if ent.label_ == "PERSON" and ent.text in valid_chars}
        sent_char_lists.append(chars_here)

    n = len(sent_char_lists)
    for start in range(0, n - window_size + 1, stride):
        win_chars = set().union(*sent_char_lists[start:start + window_size])
        if len(win_chars) < 2:
            continue
        for c1, c2 in itertools.combinations(win_chars, 2):
            matrix[c1][c2] += 1
            matrix[c2][c1] += 1
    return matrix


def split_text_into_chunks(text: str, max_words=600, max_chars=5000):
    paragraphs = re.split(r"\n\s*\n", text.strip())

    chunks = []
    curr_chunk_words = []
    curr_len_chars = 0
    curr_len_words = 0

    for para in paragraphs:
        words = para.strip().split()
        for word in words:
            word_len = len(word) + 1  # +1 for space or newline
            if (curr_len_chars + word_len > max_chars) or (curr_len_words + 1 > max_words):
                if curr_chunk_words:
                    chunks.append(" ".join(curr_chunk_words).strip())
                curr_chunk_words = [word]
                curr_len_chars = word_len
                curr_len_words = 1
            else:
                curr_chunk_words.append(word)
                curr_len_chars += word_len
                curr_len_words += 1

        # ë‹¨ë½ ê²½ê³„(ê°€ë…ìš© ë‘ ì¤„ ë°”ê¿ˆ)
        if curr_chunk_words and curr_len_chars + 2 <= max_chars:
            curr_chunk_words.append("\n\n")
            curr_len_chars += 2

    final_text = " ".join(curr_chunk_words).strip()
    if final_text:
        chunks.append(final_text)

    return chunks


def create_graph_from_matrix(matrix):
    G = nx.Graph()
    for c1, nbrs in matrix.items():
        for c2, w in nbrs.items():
            if w > 0:
                G.add_edge(c1, c2, weight=w)
    return G


def merge_graph_by_mapping(G_raw, mapping):
    G_new = nx.Graph()
    for u, v, data in G_raw.edges(data=True):
        if u not in mapping or v not in mapping:
            continue
        cu, cv = mapping[u], mapping[v]
        if cu == cv:
            continue
        w = data.get("weight", 1)
        if G_new.has_edge(cu, cv):
            G_new[cu][cv]["weight"] += w
        else:
            G_new.add_edge(cu, cv, weight=w)
    return G_new

# --------------------------------------------------
# 5. ë‹¨ì¼ ë„ì„œ ì²˜ë¦¬ -------------------------------------------------------
# --------------------------------------------------

def process_single_book(input_file):
    book_id    = os.path.splitext(os.path.basename(input_file))[0]
    gnn_folder = os.path.join("books_GNN", book_id)
    out_prefix = os.path.join(gnn_folder, f"{book_id}_part_")
    final_csv  = os.path.join("books_base", f"{book_id}.csv")

    os.makedirs(gnn_folder, exist_ok=True)
    os.makedirs("books_base", exist_ok=True)

    # íŒŒì¼ ì½ê¸° ------------------------------------------------------
    st_read = time.time()
    with open(input_file, "r", encoding="utf-8") as f:
        full_text = f.read()
    print(f"[1] íŒŒì¼ ì½ê¸° ì™„ë£Œ: {time.time() - st_read:.2f}s")

    part_len   = len(full_text) // NUM_PARTS
    text_parts = [full_text[i * part_len : (i + 1) * part_len] for i in range(NUM_PARTS - 1)]
    text_parts.append(full_text[(NUM_PARTS - 1) * part_len :])
    print(f"[1] í…ìŠ¤íŠ¸ {NUM_PARTS} íŒŒíŠ¸ ë¶„í•  ì™„ë£Œ")

    final_matrix = defaultdict(lambda: defaultdict(int))

    # íŒŒíŠ¸ë³„ ì²˜ë¦¬ ----------------------------------------------------
    for idx, part in enumerate(text_parts, 1):
        print(f"\n[íŒŒíŠ¸ {idx}] ì‹œì‘ --------")
        t_part = time.time()

        chunks = split_text_into_chunks(part, MAX_CHUNK_WORDS)
        resolved_all, character_set = "", set()

        for j, chunk in enumerate(chunks, 1):
            resolved = resolve_coreferences(chunk, predictor)
            resolved_all += resolved + "\n\n"
            character_set.update(extract_characters(resolved))
            print(f"  {j}/{len(chunks)} ì²­í¬ ì™„ë£Œ ({time.time() - t_part:.2f}s)")

        matrix = build_interaction_matrix_window(resolved_all, character_set)
        print(f"  â”œ ë§¤íŠ¸ë¦­ìŠ¤ ì™„ì„± | ì¸ë¬¼ {len(character_set)}ëª… | {time.time() - t_part:.2f}s")

        # ì´ë¦„ ì •ê·œí™” ë° í´ëŸ¬ìŠ¤í„°ë§
        norm      = {n: normalize(n) for n in matrix if normalize(n)}
        counts    = Counter(norm.values())
        groups    = cluster_names(set(norm.values()), name_counts=counts)
        mapping   = {orig: groups[norm[orig]] for orig in norm}

        G_part = merge_graph_by_mapping(create_graph_from_matrix(matrix), mapping)

        # â‘  íŒŒíŠ¸ CSV ì¦‰ì‹œ ì €ì¥ (ë…¸ë“œ ì •ë ¬ X)
        save_graph_edges_to_csv(G_part, f"{out_prefix}{idx}.csv")
        print(f"  â”” íŒŒíŠ¸ {idx} CSV ì €ì¥ ì™„ë£Œ")

        # â‘¡ íŒŒì´ë„ ë§¤íŠ¸ë¦­ìŠ¤ ëˆ„ì 
        for u, v, data in G_part.edges(data=True):
            final_matrix[u][v] += data["weight"]
            final_matrix[v][u] += data["weight"]

    # --------------------------------------------------------------
    # ìµœì¢… ê·¸ë˜í”„ ìƒì„± & ì €ì¥
    # --------------------------------------------------------------
    norm_f   = {n: normalize(n) for n in final_matrix}
    counts_f = Counter(norm_f.values())
    groups_f = cluster_names(set(norm_f.values()), name_counts=counts_f)
    mapping_f = {orig: groups_f[norm_f[orig]] for orig in norm_f}

    G_final_raw = create_graph_from_matrix(final_matrix)
    G_final     = merge_graph_by_mapping(G_final_raw, mapping_f)

    save_graph_edges_to_csv(G_final, final_csv)
    print(f"[ì™„ë£Œ] {book_id} | ë…¸ë“œ {G_final.number_of_nodes()} | ì—£ì§€ {G_final.number_of_edges()}")

# --------------------------------------------------
# 6. ì „ì²´ í´ë” ìˆœíšŒ -------------------------------------------------------
# --------------------------------------------------
PROCESSED_IDS_FILE = "processed_ids.json"


def load_processed_ids():
    if os.path.exists(PROCESSED_IDS_FILE):
        with open(PROCESSED_IDS_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()


def save_processed_ids(processed_ids):
    with open(PROCESSED_IDS_FILE, "w", encoding="utf-8") as f:
        json.dump(list(processed_ids), f, indent=2)


def batch_process_all(root_dir, genre):
    genre_path = os.path.join(root_dir, genre)
    if not os.path.isdir(genre_path):
        print(f"[ERROR] í´ë” ì—†ìŒ: {genre_path}")
        return

    processed_ids = load_processed_ids()

    for fname in os.listdir(genre_path):
        if fname.endswith(".txt"):
            if fname in processed_ids:
                print(f"[SKIP] ì´ë¯¸ ì²˜ë¦¬ë¨: {fname}")
                continue

            input_file = os.path.join(genre_path, fname)
            print(f"\n=== {genre}/{fname} ì²˜ë¦¬ ì‹œì‘ ===")
            process_single_book(input_file)

            processed_ids.add(fname)
            save_processed_ids(processed_ids)

# --------------------------------------------------
# 7. ì‹¤í–‰ ----------------------------------------------------------------
# --------------------------------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Batch process character graphs for a given genre.")
    parser.add_argument("--genre", required=True, help="Genre folder name (e.g., 'humorous_stories')")
    parser.add_argument("--root_dir", default="novel", help="Root directory (default: 'novel')")

    args = parser.parse_args()
    batch_process_all(root_dir=args.root_dir, genre=args.genre)
